{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd   \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import itertools\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal, AutoMultivariateNormal\n",
    "from pyro.infer import MCMC, NUTS, HMC, SVI, Trace_ELBO\n",
    "from pyro.optim import Adam, ClippedAdam\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# fix random generator seed (for reproducibility of results)\n",
    "np.random.seed(42)\n",
    "\n",
    "# matplotlib options\n",
    "palette = itertools.cycle(sns.color_palette())\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/df_cush.csv\")\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.DEP_DELAY\n",
    "dates = df.FL_DATE\n",
    "X = df.drop([\"DEP_DELAY\",\"FL_DATE\"],axis=1)\n",
    "X = X.drop(X.columns[336],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# standardize input features\n",
    "X_train_mean = X_train.mean(axis=0)\n",
    "X_train_std = X_train.std(axis=0)\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "\n",
    "X_test_mean = X_test.mean(axis=0)\n",
    "X_test_std = X_test.std(axis=0)\n",
    "X_test = (X_test - X_test_mean) / X_test_std\n",
    "\n",
    "# standardize target\n",
    "y_train_mean = y_train.mean()\n",
    "y_train_std = y_train.std()\n",
    "y_train = (y_train - y_train_mean) / y_train_std\n",
    "\n",
    "y_test_mean = y_test.mean()\n",
    "y_test_std = y_test.std()\n",
    "y_test = (y_test - y_test_mean) / y_test_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train: 1110920\n",
      "num test: 547171\n"
     ]
    }
   ],
   "source": [
    "print(\"num train: %d\" % len(y_train))\n",
    "print(\"num test: %d\" % len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(trues, predicted):\n",
    "    corr = np.corrcoef(predicted, trues)[0,1]\n",
    "    mae = np.mean(np.abs(predicted - trues))\n",
    "    rae = np.sum(np.abs(predicted - trues)) / np.sum(np.abs(trues - np.mean(trues)))\n",
    "    rmse = np.sqrt(np.mean((predicted - trues)**2))\n",
    "    r2 = max(0, 1 - np.sum((trues-predicted)**2) / np.sum((trues - np.mean(trues))**2))\n",
    "    return corr, mae, rae, rmse, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REALLY bad model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a model with TAXI_OUT, DESTINATION and ORIGIN as inpot with same regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, obs=None):\n",
    "    alpha = pyro.sample(\"alpha\", dist.Normal(0., 1.))                   # Prior for the bias/intercept\n",
    "    beta  = pyro.sample(\"beta\", dist.Normal(torch.zeros(X.shape[1]), \n",
    "                                            torch.ones(X.shape[1])).to_event())    # Priors for the regression coeffcients\n",
    "    sigma = pyro.sample(\"sigma\", dist.HalfCauchy(5.))                   # Prior for the variance\n",
    "    \n",
    "    with pyro.plate(\"data\"):\n",
    "        y = pyro.sample(\"y\", dist.Normal(alpha + X.matmul(beta), sigma), obs=obs)\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Pyro model\n",
    "X_train_small = torch.tensor(X_train.to_numpy()[:1000,:]).float()\n",
    "y_train_small = torch.tensor(y_train.to_numpy()[:1000]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 1200/1200 [04:08,  4.83it/s, step size=1.02e-01, acc. prob=0.902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "     alpha      0.04      0.26      0.04     -0.43      0.41    231.49      1.01\n",
      "   beta[0]      0.03      0.05      0.03     -0.05      0.10    549.60      1.00\n",
      "   beta[1]      0.08      1.01      0.06     -1.53      1.76    545.96      1.00\n",
      "   beta[2]     -0.01      0.03     -0.01     -0.05      0.04     47.21      1.06\n",
      "   beta[3]      0.07      0.96      0.12     -1.39      1.73    775.67      1.00\n",
      "   beta[4]     -0.00      0.02     -0.01     -0.03      0.02     39.04      1.07\n",
      "   beta[5]     -0.03      1.03     -0.01     -1.75      1.57   1192.25      1.00\n",
      "   beta[6]     -0.01      1.00     -0.01     -1.84      1.45   1844.27      1.00\n",
      "   beta[7]      0.00      1.00     -0.01     -1.72      1.56    688.54      1.00\n",
      "   beta[8]      0.00      0.04      0.00     -0.06      0.07     46.03      1.07\n",
      "   beta[9]     -0.02      1.04     -0.07     -1.70      1.67    557.27      1.00\n",
      "  beta[10]     -0.01      0.98     -0.02     -1.51      1.62   1047.17      1.00\n",
      "  beta[11]      0.01      0.98      0.04     -1.60      1.56   1539.44      1.00\n",
      "  beta[12]     -0.02      1.07     -0.05     -1.88      1.67   1174.57      1.00\n",
      "  beta[13]     -0.01      0.07     -0.01     -0.14      0.09     43.80      1.06\n",
      "  beta[14]      0.03      1.02      0.01     -1.59      1.74   1300.77      1.00\n",
      "  beta[15]     -0.01      0.03     -0.01     -0.06      0.05     26.08      1.11\n",
      "  beta[16]      0.07      1.03      0.08     -1.47      1.97    548.95      1.00\n",
      "  beta[17]     -0.04      1.01     -0.05     -1.59      1.75    802.46      1.01\n",
      "  beta[18]      0.03      1.03      0.08     -1.59      1.71   1090.66      1.00\n",
      "  beta[19]     -0.05      0.24     -0.03     -0.50      0.31     16.71      1.16\n",
      "  beta[20]      0.04      0.95      0.01     -1.70      1.44   1754.43      1.00\n",
      "  beta[21]     -0.01      0.11     -0.00     -0.17      0.17     19.32      1.14\n",
      "  beta[22]     -0.01      0.04     -0.01     -0.07      0.05     20.64      1.13\n",
      "  beta[23]     -0.00      0.03     -0.00     -0.06      0.05     38.39      1.07\n",
      "  beta[24]      0.03      0.97      0.05     -1.59      1.64    741.26      1.00\n",
      "  beta[25]     -0.01      0.03     -0.01     -0.06      0.04     43.43      1.06\n",
      "  beta[26]      0.02      0.07      0.02     -0.11      0.14     17.32      1.14\n",
      "  beta[27]     -0.03      1.05     -0.02     -1.67      1.66   1588.27      1.00\n",
      "  beta[28]     -0.02      0.96     -0.02     -1.77      1.35   1394.64      1.00\n",
      "  beta[29]     -0.05      1.00     -0.06     -1.52      1.78    916.94      1.00\n",
      "  beta[30]     -0.01      0.99     -0.00     -1.60      1.71   1232.33      1.00\n",
      "  beta[31]     -0.01      0.04     -0.00     -0.07      0.05     37.00      1.08\n",
      "  beta[32]     -0.01      0.08     -0.01     -0.14      0.12     41.45      1.06\n",
      "  beta[33]      0.01      1.06      0.03     -1.86      1.59   1678.31      1.00\n",
      "  beta[34]      0.00      0.03      0.00     -0.05      0.05     24.16      1.12\n",
      "  beta[35]      0.01      0.94      0.01     -1.46      1.59    441.21      1.00\n",
      "  beta[36]      0.03      1.00      0.01     -1.75      1.52    786.33      1.00\n",
      "  beta[37]      0.07      0.99      0.05     -1.55      1.62    530.48      1.00\n",
      "  beta[38]      0.02      1.01      0.00     -1.59      1.71   1927.73      1.00\n",
      "  beta[39]     -0.01      0.11      0.00     -0.21      0.15     18.57      1.15\n",
      "  beta[40]     -0.02      0.07     -0.02     -0.13      0.09     21.93      1.12\n",
      "  beta[41]     -0.02      0.15     -0.01     -0.31      0.19     18.12      1.15\n",
      "  beta[42]     -0.05      1.03     -0.09     -1.74      1.60    476.47      1.00\n",
      "  beta[43]     -0.01      0.97      0.02     -1.50      1.68   1316.07      1.00\n",
      "  beta[44]     -0.12      0.97     -0.14     -1.71      1.40    664.76      1.00\n",
      "  beta[45]     -0.05      1.02     -0.11     -1.58      1.64    835.36      1.00\n",
      "  beta[46]     -0.01      0.03     -0.01     -0.05      0.04     28.34      1.10\n",
      "  beta[47]      0.04      0.99      0.04     -1.48      1.71   1459.54      1.00\n",
      "  beta[48]     -0.04      1.01     -0.02     -1.98      1.49    743.37      1.00\n",
      "  beta[49]      0.00      1.03      0.01     -1.73      1.61    370.80      1.00\n",
      "  beta[50]     -0.01      0.06     -0.01     -0.11      0.08     32.65      1.07\n",
      "  beta[51]     -0.04      0.08     -0.03     -0.15      0.09     25.28      1.12\n",
      "  beta[52]     -0.03      0.09     -0.03     -0.19      0.11     28.98      1.10\n",
      "  beta[53]     -0.02      0.13     -0.02     -0.26      0.19     17.83      1.15\n",
      "  beta[54]     -0.01      0.05     -0.01     -0.09      0.08     43.08      1.08\n",
      "  beta[55]     -0.01      0.04     -0.01     -0.08      0.05     26.34      1.11\n",
      "  beta[56]     -0.02      0.05     -0.02     -0.11      0.06     39.03      1.07\n",
      "  beta[57]     -0.07      0.99     -0.10     -1.58      1.64    685.48      1.00\n",
      "  beta[58]      0.00      1.04     -0.02     -1.74      1.75   1437.04      1.00\n",
      "  beta[59]      0.03      0.99      0.03     -1.46      1.75    754.11      1.00\n",
      "  beta[60]     -0.02      0.05     -0.01     -0.09      0.06     28.08      1.10\n",
      "  beta[61]     -0.01      0.05     -0.01     -0.08      0.08     42.72      1.07\n",
      "  beta[62]     -0.01      0.06     -0.01     -0.14      0.08     21.73      1.13\n",
      "  beta[63]     -0.01      0.05     -0.01     -0.10      0.07     31.68      1.09\n",
      "  beta[64]     -0.02      1.02     -0.02     -1.76      1.64   1081.17      1.00\n",
      "  beta[65]     -0.03      1.03     -0.10     -1.66      1.76    817.71      1.00\n",
      "  beta[66]      0.01      0.10      0.02     -0.16      0.15     19.99      1.14\n",
      "  beta[67]      0.05      0.94      0.01     -1.49      1.69   1291.03      1.00\n",
      "  beta[68]     -0.04      0.19     -0.02     -0.40      0.24     18.12      1.16\n",
      "  beta[69]     -0.01      0.10     -0.01     -0.20      0.13     24.22      1.12\n",
      "  beta[70]     -0.05      1.02     -0.07     -1.72      1.61    939.15      1.00\n",
      "  beta[71]     -0.00      1.03     -0.00     -1.73      1.60   1800.02      1.00\n",
      "  beta[72]      0.01      1.02      0.03     -1.69      1.58    667.86      1.00\n",
      "  beta[73]      0.00      0.05      0.00     -0.10      0.09     30.75      1.08\n",
      "  beta[74]      0.04      0.98      0.05     -1.51      1.63    768.50      1.00\n",
      "  beta[75]     -0.08      0.99     -0.10     -1.78      1.49    749.80      1.00\n",
      "  beta[76]      0.06      0.99      0.08     -1.45      1.73    820.95      1.00\n",
      "  beta[77]      0.00      0.04      0.00     -0.07      0.07     37.97      1.08\n",
      "  beta[78]     -0.00      0.99      0.00     -1.74      1.46    877.53      1.00\n",
      "  beta[79]     -0.02      0.09     -0.02     -0.19      0.12     19.35      1.14\n",
      "  beta[80]     -0.01      0.03     -0.01     -0.06      0.04     44.08      1.06\n",
      "  beta[81]     -0.00      0.96      0.02     -1.62      1.47   1623.62      1.00\n",
      "  beta[82]     -0.01      0.11     -0.01     -0.23      0.14     17.11      1.16\n",
      "  beta[83]      0.04      0.06      0.04     -0.04      0.14     25.29      1.11\n",
      "  beta[84]      0.02      1.02      0.05     -1.63      1.70   1236.88      1.00\n",
      "  beta[85]     -0.03      0.14     -0.01     -0.29      0.18     16.78      1.16\n",
      "  beta[86]      0.01      0.19      0.03     -0.34      0.29     18.97      1.15\n",
      "  beta[87]     -0.06      0.21     -0.04     -0.44      0.26     16.88      1.16\n",
      "  beta[88]      0.02      0.94      0.01     -1.67      1.38    778.72      1.00\n",
      "  beta[89]     -0.01      0.03     -0.00     -0.05      0.04     30.43      1.09\n",
      "  beta[90]     -0.01      0.96      0.01     -1.49      1.68   1925.73      1.00\n",
      "  beta[91]      0.01      0.06      0.01     -0.10      0.11     28.99      1.09\n",
      "  beta[92]     -0.01      0.16     -0.00     -0.30      0.24     17.30      1.17\n",
      "  beta[93]      0.01      0.98      0.01     -1.61      1.56    486.20      1.00\n",
      "  beta[94]     -0.00      1.03     -0.01     -1.68      1.65    867.92      1.01\n",
      "  beta[95]      0.05      0.99      0.05     -1.39      1.80    948.56      1.00\n",
      "  beta[96]      0.03      0.03      0.03     -0.03      0.08     23.23      1.13\n",
      "  beta[97]      0.00      1.00      0.02     -1.80      1.44    539.90      1.00\n",
      "  beta[98]      0.01      0.97      0.02     -1.62      1.52   1141.80      1.00\n",
      "  beta[99]     -0.01      0.05     -0.01     -0.12      0.07     22.01      1.12\n",
      " beta[100]     -0.00      1.01     -0.02     -1.49      1.79   1105.84      1.00\n",
      " beta[101]     -0.04      1.06     -0.02     -1.69      1.75   1074.19      1.00\n",
      " beta[102]      0.01      0.99     -0.02     -1.55      1.60   1428.68      1.00\n",
      " beta[103]     -0.06      0.96     -0.04     -1.59      1.50    662.74      1.00\n",
      " beta[104]     -0.00      1.03     -0.05     -1.70      1.68   1578.51      1.00\n",
      " beta[105]      0.04      0.15      0.06     -0.25      0.26     16.94      1.16\n",
      " beta[106]     -0.06      1.00     -0.05     -1.61      1.66    472.81      1.00\n",
      " beta[107]      0.03      0.97      0.02     -1.51      1.64   1154.51      1.00\n",
      " beta[108]     -0.02      1.02     -0.03     -1.56      1.77   1272.84      1.00\n",
      " beta[109]     -0.02      0.05     -0.02     -0.11      0.06     27.97      1.10\n",
      " beta[110]     -0.02      0.04     -0.02     -0.08      0.05     43.67      1.06\n",
      " beta[111]     -0.01      0.03     -0.01     -0.05      0.03     35.61      1.07\n",
      " beta[112]     -0.09      1.01     -0.07     -1.70      1.55    458.91      1.00\n",
      " beta[113]      0.02      0.14      0.02     -0.22      0.25     19.71      1.14\n",
      " beta[114]      0.02      1.02     -0.01     -1.61      1.75   1068.15      1.00\n",
      " beta[115]     -0.01      0.04     -0.01     -0.08      0.05     41.67      1.06\n",
      " beta[116]     -0.02      0.05     -0.02     -0.09      0.07     42.12      1.07\n",
      " beta[117]     -0.04      0.97     -0.01     -1.61      1.50    721.40      1.00\n",
      " beta[118]     -0.00      0.99      0.02     -1.58      1.57   1151.61      1.00\n",
      " beta[119]     -0.03      1.02     -0.01     -1.68      1.62    837.68      1.00\n",
      " beta[120]     -0.01      1.00     -0.01     -1.63      1.66   1426.76      1.00\n",
      " beta[121]     -0.02      0.06     -0.02     -0.11      0.08     26.21      1.12\n",
      " beta[122]      0.04      1.00      0.04     -1.49      1.70   1415.11      1.00\n",
      " beta[123]      0.02      1.00      0.01     -1.57      1.74    680.13      1.00\n",
      " beta[124]     -0.03      0.99     -0.02     -1.69      1.48   1150.18      1.00\n",
      " beta[125]     -0.01      0.03     -0.01     -0.08      0.04     28.98      1.08\n",
      " beta[126]     -0.01      0.03     -0.02     -0.08      0.03     32.09      1.08\n",
      " beta[127]     -0.02      0.04     -0.01     -0.08      0.05     43.62      1.05\n",
      " beta[128]      0.00      0.99      0.04     -1.40      1.76    678.61      1.00\n",
      " beta[129]      0.01      1.08     -0.01     -1.82      1.74    938.01      1.00\n",
      " beta[130]     -0.02      0.06     -0.02     -0.13      0.07     22.95      1.12\n",
      " beta[131]      0.02      0.96      0.03     -1.46      1.66    437.24      1.00\n",
      " beta[132]      0.09      1.03      0.13     -1.55      1.74    706.39      1.00\n",
      " beta[133]      0.02      0.97     -0.02     -1.58      1.61    770.23      1.00\n",
      " beta[134]      0.00      1.01     -0.03     -1.60      1.64    451.62      1.00\n",
      " beta[135]     -0.00      0.98     -0.02     -1.66      1.61   1105.61      1.00\n",
      " beta[136]      0.05      0.97      0.05     -1.70      1.55    909.82      1.00\n",
      " beta[137]      0.02      1.01      0.05     -1.71      1.62    734.42      1.00\n",
      " beta[138]     -0.04      0.96     -0.02     -1.68      1.43    918.41      1.00\n",
      " beta[139]      0.06      1.00      0.06     -1.56      1.63    747.07      1.00\n",
      " beta[140]      0.01      1.02      0.02     -1.53      1.73   2430.07      1.00\n",
      " beta[141]      0.04      0.10      0.04     -0.13      0.20     19.38      1.13\n",
      " beta[142]     -0.02      0.99     -0.00     -1.83      1.45   2085.65      1.00\n",
      " beta[143]     -0.01      0.10      0.00     -0.19      0.16     20.18      1.14\n",
      " beta[144]      0.05      0.97      0.00     -1.43      1.74   1177.16      1.00\n",
      " beta[145]     -0.01      0.03     -0.01     -0.07      0.03     25.07      1.11\n",
      " beta[146]      0.04      0.99      0.04     -1.56      1.66    799.18      1.00\n",
      " beta[147]     -0.00      0.01     -0.00     -0.02      0.02     34.53      1.09\n",
      " beta[148]      0.00      0.02      0.00     -0.03      0.03     47.45      1.06\n",
      " beta[149]     -0.01      1.00     -0.01     -1.55      1.73    568.55      1.00\n",
      " beta[150]     -0.04      0.10     -0.03     -0.22      0.14     18.99      1.15\n",
      " beta[151]      0.01      0.99     -0.02     -1.42      1.87    783.43      1.00\n",
      " beta[152]     -0.05      0.17     -0.04     -0.34      0.21     18.16      1.16\n",
      " beta[153]      0.07      0.05      0.07     -0.02      0.15     30.04      1.09\n",
      " beta[154]     -0.01      0.03     -0.01     -0.07      0.04     34.37      1.08\n",
      " beta[155]     -0.02      1.00     -0.00     -1.57      1.63   1327.24      1.00\n",
      " beta[156]      0.03      0.05      0.03     -0.04      0.11     37.52      1.06\n",
      " beta[157]     -0.02      1.03     -0.00     -1.89      1.49    830.39      1.00\n",
      " beta[158]     -0.03      0.10     -0.02     -0.19      0.14     20.71      1.13\n",
      " beta[159]      0.03      0.97      0.05     -1.59      1.48   1134.43      1.00\n",
      " beta[160]      0.02      1.02     -0.02     -1.66      1.70   1162.66      1.00\n",
      " beta[161]     -0.01      1.01     -0.00     -1.65      1.69   1499.28      1.00\n",
      " beta[162]     -0.08      0.98     -0.07     -1.57      1.55    329.37      1.00\n",
      " beta[163]      0.02      0.04      0.02     -0.05      0.09     27.92      1.09\n",
      " beta[164]      0.01      1.00     -0.02     -1.55      1.71    629.64      1.00\n",
      " beta[165]      0.00      0.98     -0.04     -1.53      1.64    684.09      1.00\n",
      " beta[166]     -0.02      0.09     -0.01     -0.14      0.14     28.28      1.09\n",
      " beta[167]      0.10      0.14      0.12     -0.14      0.33     17.80      1.15\n",
      " beta[168]     -0.07      0.97     -0.12     -1.72      1.42    380.58      1.00\n",
      " beta[169]     -0.01      0.02     -0.01     -0.04      0.03     44.88      1.05\n",
      " beta[170]     -0.02      0.03     -0.02     -0.08      0.03     30.19      1.09\n",
      " beta[171]      0.21      0.06      0.21      0.10      0.30     26.57      1.11\n",
      " beta[172]     -0.01      1.06     -0.04     -1.64      1.75   1934.46      1.00\n",
      " beta[173]     -0.01      0.03     -0.01     -0.06      0.04     28.12      1.10\n",
      " beta[174]      0.07      1.01      0.05     -1.60      1.59    719.02      1.00\n",
      " beta[175]     -0.02      0.16     -0.01     -0.32      0.22     17.70      1.16\n",
      " beta[176]     -0.02      1.01      0.03     -1.73      1.55    675.36      1.00\n",
      " beta[177]     -0.00      0.19      0.01     -0.32      0.31     17.81      1.15\n",
      " beta[178]     -0.01      0.04     -0.01     -0.08      0.06     34.85      1.06\n",
      " beta[179]     -0.02      1.02     -0.03     -1.66      1.62    520.04      1.00\n",
      " beta[180]     -0.00      1.01     -0.05     -1.58      1.66    898.73      1.00\n",
      " beta[181]     -0.00      0.01     -0.00     -0.03      0.02     51.55      1.06\n",
      " beta[182]      0.01      1.01      0.00     -1.56      1.67   1181.61      1.00\n",
      " beta[183]     -0.04      0.97     -0.04     -1.64      1.55    739.65      1.00\n",
      " beta[184]      0.04      0.98      0.04     -1.60      1.61    756.19      1.00\n",
      " beta[185]     -0.01      0.98     -0.02     -1.54      1.67    722.71      1.00\n",
      " beta[186]      0.03      0.16      0.04     -0.27      0.28     15.96      1.17\n",
      " beta[187]     -0.00      0.06     -0.00     -0.12      0.09     23.63      1.12\n",
      " beta[188]      0.02      0.06      0.02     -0.09      0.10     25.09      1.12\n",
      " beta[189]     -0.01      0.07     -0.01     -0.14      0.10     42.75      1.07\n",
      " beta[190]      0.00      1.03     -0.05     -1.73      1.68    531.53      1.00\n",
      " beta[191]     -0.01      1.05     -0.00     -1.65      1.72   1608.12      1.00\n",
      " beta[192]     -0.01      0.03     -0.01     -0.05      0.03     46.64      1.06\n",
      " beta[193]      0.02      0.98      0.00     -1.66      1.53    458.76      1.00\n",
      " beta[194]     -0.00      0.97      0.03     -1.48      1.72   1061.04      1.00\n",
      " beta[195]     -0.02      0.06     -0.02     -0.12      0.06     43.15      1.06\n",
      " beta[196]     -0.02      0.95      0.01     -1.50      1.59   1899.21      1.00\n",
      " beta[197]     -0.03      0.10     -0.02     -0.22      0.11     20.04      1.14\n",
      " beta[198]      0.01      0.16      0.02     -0.30      0.21     17.32      1.16\n",
      " beta[199]      0.06      1.01      0.06     -1.68      1.72    315.41      1.00\n",
      " beta[200]     -0.03      0.12     -0.02     -0.22      0.17     18.43      1.15\n",
      " beta[201]      0.01      1.09     -0.02     -1.62      1.91   1222.42      1.00\n",
      " beta[202]     -0.02      0.07     -0.01     -0.13      0.09     21.18      1.14\n",
      " beta[203]     -0.01      0.04     -0.01     -0.07      0.06     44.45      1.06\n",
      " beta[204]      0.02      0.04      0.02     -0.04      0.08     39.39      1.07\n",
      " beta[205]      0.04      1.05      0.04     -1.75      1.70   3393.24      1.00\n",
      " beta[206]     -0.00      1.00      0.02     -1.70      1.53    763.97      1.00\n",
      " beta[207]      0.00      1.02     -0.01     -1.58      1.62   1814.94      1.00\n",
      " beta[208]     -0.03      0.13     -0.02     -0.27      0.16     17.03      1.15\n",
      " beta[209]     -0.01      0.08     -0.00     -0.15      0.12     21.47      1.12\n",
      " beta[210]      0.02      1.05      0.02     -1.74      1.71    791.97      1.00\n",
      " beta[211]      0.01      0.98      0.01     -1.64      1.64   2181.99      1.00\n",
      " beta[212]     -0.01      0.04     -0.01     -0.08      0.07     52.84      1.05\n",
      " beta[213]      0.19      0.04      0.19      0.13      0.25     42.67      1.07\n",
      " beta[214]      0.00      0.95      0.00     -1.60      1.45    622.02      1.00\n",
      " beta[215]     -0.01      0.04     -0.01     -0.08      0.06     28.34      1.10\n",
      " beta[216]     -0.01      0.03     -0.01     -0.06      0.03     47.81      1.06\n",
      " beta[217]      0.02      0.99     -0.01     -1.42      1.84   1882.57      1.00\n",
      " beta[218]      0.00      0.99      0.02     -1.69      1.65    808.23      1.00\n",
      " beta[219]     -0.10      1.00     -0.10     -1.61      1.66    563.60      1.00\n",
      " beta[220]     -0.01      0.98     -0.03     -1.86      1.40   1066.89      1.00\n",
      " beta[221]      0.02      0.16      0.03     -0.28      0.24     16.98      1.16\n",
      " beta[222]     -0.03      0.11     -0.03     -0.21      0.15     19.21      1.14\n",
      " beta[223]      0.02      0.98      0.05     -1.48      1.73    759.07      1.00\n",
      " beta[224]     -0.01      0.05     -0.01     -0.09      0.06     34.15      1.08\n",
      " beta[225]     -0.01      0.03     -0.01     -0.07      0.04     47.29      1.05\n",
      " beta[226]     -0.01      0.10     -0.00     -0.19      0.13     18.00      1.15\n",
      " beta[227]     -0.04      1.03      0.01     -1.59      1.89    734.15      1.00\n",
      " beta[228]      0.03      0.08      0.03     -0.10      0.17     26.80      1.10\n",
      " beta[229]      0.08      1.00      0.11     -1.55      1.80    498.34      1.01\n",
      " beta[230]      0.00      0.07      0.00     -0.10      0.12     19.13      1.14\n",
      " beta[231]     -0.04      0.10     -0.04     -0.21      0.12     45.95      1.06\n",
      " beta[232]      0.01      1.00      0.05     -1.73      1.52    831.96      1.00\n",
      " beta[233]     -0.02      0.07     -0.02     -0.14      0.09     20.08      1.14\n",
      " beta[234]     -0.01      0.22      0.00     -0.38      0.36     17.24      1.17\n",
      " beta[235]     -0.02      0.07     -0.02     -0.13      0.09     27.01      1.09\n",
      " beta[236]      0.01      0.99     -0.01     -1.49      1.72   1451.27      1.00\n",
      " beta[237]     -0.01      1.03     -0.01     -1.53      1.74   1660.65      1.00\n",
      " beta[238]     -0.01      1.00     -0.02     -1.51      1.71   1231.08      1.00\n",
      " beta[239]     -0.02      1.00      0.01     -1.51      1.66    824.76      1.00\n",
      " beta[240]     -0.02      0.99     -0.00     -1.65      1.56    787.70      1.00\n",
      " beta[241]      0.02      1.03      0.01     -1.66      1.61    609.27      1.00\n",
      " beta[242]     -0.01      0.08     -0.01     -0.17      0.11     22.20      1.12\n",
      " beta[243]     -0.02      0.11     -0.01     -0.20      0.16     20.48      1.14\n",
      " beta[244]     -0.01      0.05     -0.01     -0.10      0.08     46.23      1.07\n",
      " beta[245]     -0.01      0.99     -0.05     -1.56      1.63    600.45      1.00\n",
      " beta[246]     -0.04      0.14     -0.03     -0.27      0.18     18.63      1.16\n",
      " beta[247]     -0.02      0.17     -0.00     -0.32      0.27     16.68      1.16\n",
      " beta[248]     -0.02      0.05     -0.02     -0.09      0.07     42.49      1.07\n",
      " beta[249]      0.00      0.02      0.00     -0.03      0.03     49.93      1.06\n",
      " beta[250]     -0.02      0.06     -0.02     -0.11      0.08     44.00      1.07\n",
      " beta[251]     -0.00      1.00      0.01     -1.63      1.54   1186.29      1.00\n",
      " beta[252]     -0.03      0.10     -0.03     -0.20      0.12     22.33      1.11\n",
      " beta[253]      0.02      1.00      0.01     -1.57      1.61    983.10      1.00\n",
      " beta[254]     -0.01      0.05     -0.01     -0.10      0.07     29.44      1.09\n",
      " beta[255]     -0.00      1.02     -0.03     -1.66      1.66   1633.04      1.00\n",
      " beta[256]     -0.02      0.03     -0.01     -0.07      0.04     30.19      1.09\n",
      " beta[257]     -0.05      0.98     -0.07     -1.63      1.58    758.36      1.00\n",
      " beta[258]      0.03      0.93      0.03     -1.44      1.58    762.43      1.00\n",
      " beta[259]      0.03      1.05      0.06     -1.71      1.71    854.52      1.00\n",
      " beta[260]      0.01      0.06      0.02     -0.09      0.10     25.56      1.11\n",
      " beta[261]      0.02      1.00      0.03     -1.76      1.48    961.57      1.00\n",
      " beta[262]     -0.02      0.06     -0.02     -0.13      0.08     26.07      1.09\n",
      " beta[263]      0.02      0.98      0.05     -1.54      1.62   1047.71      1.00\n",
      " beta[264]      0.01      0.95      0.02     -1.50      1.64   1341.90      1.00\n",
      " beta[265]     -0.01      0.04     -0.01     -0.07      0.05     41.09      1.06\n",
      " beta[266]     -0.01      0.02     -0.01     -0.04      0.03     49.48      1.05\n",
      " beta[267]      0.02      1.00     -0.00     -1.55      1.71    929.71      1.00\n",
      " beta[268]     -0.03      0.10     -0.03     -0.22      0.11     17.34      1.15\n",
      " beta[269]      0.04      1.00      0.04     -1.54      1.63    704.31      1.00\n",
      " beta[270]     -0.01      0.02     -0.01     -0.03      0.02     38.65      1.06\n",
      " beta[271]      0.02      0.07      0.02     -0.10      0.14     25.17      1.11\n",
      " beta[272]     -0.03      0.96     -0.02     -1.65      1.48    611.16      1.00\n",
      " beta[273]     -0.02      0.06     -0.02     -0.12      0.08     23.29      1.12\n",
      " beta[274]     -0.03      1.03     -0.03     -1.72      1.54    562.12      1.00\n",
      " beta[275]     -0.01      0.07     -0.01     -0.14      0.09     44.18      1.06\n",
      " beta[276]     -0.00      0.02     -0.00     -0.04      0.04     52.05      1.05\n",
      " beta[277]      0.02      0.03      0.02     -0.03      0.06     28.52      1.10\n",
      " beta[278]      0.34      0.10      0.34      0.17      0.49     17.82      1.14\n",
      " beta[279]      0.04      0.95      0.01     -1.60      1.42    369.53      1.00\n",
      " beta[280]     -0.03      0.12     -0.02     -0.27      0.14     20.63      1.13\n",
      " beta[281]     -0.03      0.08     -0.02     -0.16      0.11     18.71      1.14\n",
      " beta[282]     -0.02      0.07     -0.02     -0.12      0.10     56.50      1.04\n",
      " beta[283]     -0.02      0.05     -0.02     -0.10      0.07     48.61      1.05\n",
      " beta[284]      0.01      0.99      0.03     -1.63      1.59    997.74      1.00\n",
      " beta[285]      0.02      1.00      0.02     -1.53      1.72    495.22      1.00\n",
      " beta[286]      0.03      1.00      0.06     -1.68      1.47    470.49      1.00\n",
      " beta[287]     -0.03      0.99     -0.04     -1.64      1.55    718.67      1.00\n",
      " beta[288]     -0.04      1.04     -0.07     -1.63      1.76    885.77      1.00\n",
      " beta[289]      0.01      0.96      0.02     -1.64      1.46    435.71      1.00\n",
      " beta[290]     -0.05      0.15     -0.04     -0.33      0.16     18.46      1.15\n",
      " beta[291]     -0.01      0.07     -0.01     -0.10      0.12     37.46      1.08\n",
      " beta[292]     -0.03      0.16     -0.02     -0.29      0.27     16.93      1.16\n",
      " beta[293]     -0.02      0.05     -0.02     -0.10      0.05     29.41      1.09\n",
      " beta[294]      0.01      1.00     -0.02     -1.47      1.70   1532.68      1.00\n",
      " beta[295]     -0.02      1.00     -0.03     -1.59      1.57    891.47      1.00\n",
      " beta[296]      0.00      1.01     -0.05     -1.66      1.60   1127.70      1.00\n",
      " beta[297]      0.06      0.10      0.06     -0.13      0.21     21.44      1.13\n",
      " beta[298]      0.03      1.01      0.00     -1.46      1.81    815.63      1.00\n",
      " beta[299]      0.02      0.07      0.02     -0.09      0.14     22.28      1.12\n",
      " beta[300]      0.01      0.14      0.02     -0.22      0.24     16.27      1.16\n",
      " beta[301]     -0.03      0.09     -0.02     -0.21      0.09     17.77      1.15\n",
      " beta[302]      0.01      1.00      0.04     -1.61      1.68   1114.89      1.00\n",
      " beta[303]     -0.02      0.08     -0.02     -0.16      0.12     18.98      1.14\n",
      " beta[304]      0.01      0.97      0.03     -1.41      1.66   1281.72      1.00\n",
      " beta[305]      0.04      1.04      0.06     -1.67      1.67    869.37      1.00\n",
      " beta[306]     -0.01      0.05     -0.01     -0.10      0.07     48.63      1.06\n",
      " beta[307]      0.05      0.99      0.03     -1.56      1.73    932.69      1.00\n",
      " beta[308]     -0.01      0.11     -0.00     -0.19      0.16     17.98      1.15\n",
      " beta[309]     -0.02      1.02     -0.05     -1.67      1.66    696.65      1.00\n",
      " beta[310]      0.09      1.01      0.08     -1.68      1.53    735.37      1.00\n",
      " beta[311]      0.04      1.00      0.03     -1.50      1.74    866.81      1.00\n",
      " beta[312]      0.01      0.97      0.00     -1.53      1.61    728.62      1.00\n",
      " beta[313]     -0.05      1.03     -0.02     -1.78      1.57   1258.17      1.00\n",
      " beta[314]      0.02      1.05      0.02     -1.66      1.67   1258.71      1.00\n",
      " beta[315]      0.02      1.00     -0.01     -1.54      1.71    891.19      1.00\n",
      " beta[316]     -0.03      0.06     -0.03     -0.13      0.07     24.75      1.11\n",
      " beta[317]     -0.02      0.04     -0.02     -0.08      0.05     29.13      1.10\n",
      " beta[318]      0.07      0.92      0.07     -1.41      1.60   1404.85      1.00\n",
      " beta[319]     -0.02      0.12     -0.01     -0.24      0.16     17.74      1.16\n",
      " beta[320]     -0.01      0.04     -0.01     -0.07      0.05     44.14      1.07\n",
      " beta[321]     -0.01      0.99      0.00     -1.58      1.59    885.70      1.00\n",
      " beta[322]     -0.02      0.08     -0.01     -0.15      0.12     43.43      1.05\n",
      " beta[323]     -0.02      0.07     -0.02     -0.13      0.09     25.30      1.11\n",
      " beta[324]      0.01      0.94      0.00     -1.40      1.68    444.22      1.00\n",
      " beta[325]     -0.00      0.02     -0.00     -0.04      0.02     25.42      1.10\n",
      " beta[326]     -0.03      0.98     -0.04     -1.51      1.55    930.62      1.00\n",
      " beta[327]     -0.04      0.93     -0.06     -1.74      1.26   1681.52      1.00\n",
      " beta[328]     -0.01      0.07     -0.01     -0.12      0.12     40.63      1.07\n",
      " beta[329]      0.00      0.97     -0.02     -1.48      1.62   1661.83      1.00\n",
      " beta[330]      0.03      1.03     -0.01     -1.49      1.90    877.43      1.00\n",
      " beta[331]     -0.01      0.02     -0.01     -0.04      0.03     42.15      1.06\n",
      " beta[332]     -0.01      1.01      0.03     -1.69      1.61   1271.02      1.00\n",
      " beta[333]      0.01      1.00      0.03     -1.68      1.53   1751.19      1.00\n",
      " beta[334]     -0.01      0.07     -0.01     -0.13      0.10     41.50      1.06\n",
      " beta[335]     -0.03      0.95     -0.06     -1.67      1.46    893.92      1.00\n",
      " beta[336]     -0.03      0.98     -0.02     -1.73      1.46    982.71      1.00\n",
      " beta[337]      0.03      0.20      0.04     -0.28      0.32     17.25      1.17\n",
      " beta[338]     -0.01      0.37     -0.01     -0.58      0.57     16.03      1.18\n",
      " beta[339]      0.01      0.18      0.02     -0.27      0.31     16.90      1.16\n",
      " beta[340]     -0.05      0.22     -0.05     -0.41      0.29     16.96      1.17\n",
      " beta[341]     -0.05      0.37     -0.04     -0.62      0.54     16.27      1.17\n",
      " beta[342]     -0.03      0.20     -0.02     -0.35      0.28     17.09      1.16\n",
      " beta[343]     -0.09      0.14     -0.08     -0.31      0.14     17.05      1.16\n",
      " beta[344]     -0.02      0.15     -0.02     -0.27      0.22     19.97      1.15\n",
      " beta[345]     -0.12      0.14     -0.11     -0.33      0.12     19.67      1.14\n",
      " beta[346]     -0.04      0.22     -0.03     -0.37      0.30     16.94      1.16\n",
      " beta[347]     -0.10      0.17     -0.09     -0.37      0.18     17.06      1.16\n",
      " beta[348]      0.02      0.22      0.03     -0.34      0.34     17.58      1.15\n",
      " beta[349]     -0.01      0.34      0.00     -0.53      0.52     16.15      1.18\n",
      " beta[350]     -0.05      0.30     -0.04     -0.50      0.44     16.29      1.17\n",
      " beta[351]     -0.05      0.12     -0.04     -0.24      0.14     18.53      1.14\n",
      " beta[352]     -0.01      0.43      0.01     -0.65      0.67     16.01      1.17\n",
      " beta[353]     -0.02      0.18     -0.01     -0.34      0.24     16.70      1.17\n",
      " beta[354]     -0.06      0.23     -0.06     -0.41      0.30     16.46      1.17\n",
      "     sigma      1.28      0.03      1.28      1.23      1.33    912.56      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "# Run inference in Pyro\n",
    "nuts_kernel = NUTS(model)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=1000, warmup_steps=200, num_chains=1)\n",
    "mcmc.run(X_train_small, y_train_small)\n",
    "\n",
    "# Show summary of inference results\n",
    "mcmc.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CorrCoef: 0.005\n",
      "MAE: 25.340\n",
      "RMSE: 53.288\n",
      "R2: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Extract samples from posterior\n",
    "posterior_samples = mcmc.get_samples()\n",
    "\n",
    "# Compute predictions\n",
    "y_hat = np.mean(posterior_samples[\"alpha\"].numpy().T + np.dot(X_test.to_numpy(), posterior_samples[\"beta\"].numpy().T), axis=1)\n",
    "\n",
    "# Convert back to the original scale\n",
    "preds = y_hat * y_test_std + y_test_mean\n",
    "y_true = y_test * y_test_std + y_test_mean\n",
    "\n",
    "corr, mae, rae, rmse, r2 = compute_error(y_true, preds)\n",
    "print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr, mae, rmse, r2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try very little subset of data\n",
    "Xnn = X.iloc[:1000,:]\n",
    "ynn = y.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xnn, ynn, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Try Neural Network model\n",
    "Xnn = torch.tensor(Xnn.to_numpy()).float()\n",
    "ynn = torch.tensor(ynn.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FFNN(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # Architecture\n",
    "        self.in_layer = torch.nn.Linear(n_in, n_hidden)\n",
    "        self.h_layer = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.out_layer = torch.nn.Linear(n_hidden, n_out)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        X = self.tanh(self.in_layer(X))\n",
    "        X = self.tanh(self.h_layer(X))\n",
    "        X = self.out_layer(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnet_model(X, y=None):\n",
    "    # Initialize the neural network from PyTorch \n",
    "    torch_model = FFNN(n_in=X.shape[1], n_hidden=4, n_out=1) \n",
    "    \n",
    "    # Convert the PyTorch neural net into a Pyro model with priors\n",
    "    priors = {} # Priors for the neural model\n",
    "    for name, par in torch_model.named_parameters():     # Loop over all neural network parameters\n",
    "        priors[name] = dist.Normal(torch.zeros(*par.shape), torch.ones(*par.shape)).to_event() # Each parameter has a N(0, 1) prior\n",
    "    \n",
    "    bayesian_model = pyro.random_module('bayesian_model', torch_model, priors) # Make this model and these priors a Pyro model\n",
    "    sampled_model = bayesian_model()                                           # Initialize the model\n",
    "    \n",
    "    # The generative process\n",
    "    with pyro.plate(\"observations\"):\n",
    "        prediction_mean = sampled_model(X).squeeze(-1) # Feed-forward the design matrix X through the neural network\n",
    "        y = pyro.sample(\"obs\", dist.Normal(prediction_mean, 0.1), obs=y)\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define guide function\n",
    "guide = AutoDiagonalNormal(nnet_model)\n",
    "\n",
    "# Reset parameter values\n",
    "pyro.clear_param_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\anaconda3\\lib\\site-packages\\pyro\\primitives.py:491: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ELBO: 82530294.2\n",
      "[500] ELBO: 70993898.2\n",
      "[1000] ELBO: 67977564.9\n",
      "[1500] ELBO: 64977375.5\n",
      "[2000] ELBO: 61940984.1\n",
      "[2500] ELBO: 59908025.4\n",
      "[3000] ELBO: 58302806.4\n",
      "[3500] ELBO: 56723692.4\n",
      "[4000] ELBO: 55365883.0\n",
      "[4500] ELBO: 54582598.2\n",
      "[5000] ELBO: 53912903.9\n",
      "[5500] ELBO: 53387126.5\n",
      "[6000] ELBO: 52998944.7\n",
      "[6500] ELBO: 52776861.7\n",
      "[7000] ELBO: 52427180.9\n",
      "[7500] ELBO: 52168328.4\n",
      "[8000] ELBO: 52001029.2\n",
      "[8500] ELBO: 51913061.6\n",
      "[9000] ELBO: 51767192.4\n",
      "[9500] ELBO: 51681162.7\n"
     ]
    }
   ],
   "source": [
    "# Define the number of optimization steps\n",
    "n_steps = 10000\n",
    "\n",
    "# Setup the optimizer\n",
    "adam_params = {\"lr\": 0.01}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "# Setup the inference algorithm\n",
    "elbo = Trace_ELBO(num_particles=1)\n",
    "svi = SVI(nnet_model, guide, optimizer, loss=elbo)\n",
    "\n",
    "# Do gradient steps\n",
    "for step in range(n_steps):\n",
    "    elbo = svi.step(Xnn, ynn)\n",
    "    if step % 500 == 0:\n",
    "        print(\"[%d] ELBO: %.1f\" % (step, elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data for Pyro\n",
    "X_test = torch.tensor(X_test.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\anaconda3\\lib\\site-packages\\pyro\\primitives.py:491: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "# Make predictions for test set\n",
    "predictive = Predictive(nnet_model, guide=guide, num_samples=1000,\n",
    "                        return_sites=(\"obs\", \"_RETURN\"))\n",
    "samples = predictive(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnet_interpretable_model(X, y=None):\n",
    "    # Initialize the neural network from PyTorch \n",
    "    torch_model = FFNN(n_in=X.shape[1]-1, n_hidden=4, n_out=1) \n",
    "    \n",
    "    # Convert the PyTorch neural net into a Pyro model with priors\n",
    "    priors = {} # Priors for the neural model\n",
    "    for name, par in torch_model.named_parameters():     # Loop over all neural network parameters\n",
    "        priors[name] = dist.Normal(torch.zeros(*par.shape), torch.ones(*par.shape)).to_event() # Each parameter has a N(0, 1) prior\n",
    "    \n",
    "    bayesian_model = pyro.random_module('bayesian_model', torch_model, priors) # Make this model and these priors a Pyro model\n",
    "    sampled_model = bayesian_model()                                           # Initialize the model\n",
    "    \n",
    "    # Linear model priors\n",
    "    beta_lin = pyro.sample(\"beta\", dist.Normal(0, 1))\n",
    "    \n",
    "    # The generative process\n",
    "    with pyro.plate(\"observations\"):\n",
    "        linear_out = X[:,0]*beta_lin\n",
    "        nn_out = sampled_model(X[:,1:]).squeeze(-1) # Feed-forward the design matrix X through the neural network\n",
    "        y = pyro.sample(\"obs\", dist.Normal(linear_out+nn_out, 0.1), obs=y)\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ELBO: 79205579.7\n",
      "[500] ELBO: 70457553.3\n",
      "[1000] ELBO: 69417837.9\n",
      "[1500] ELBO: 68921620.3\n",
      "[2000] ELBO: 68660881.3\n",
      "[2500] ELBO: 68553536.2\n",
      "[3000] ELBO: 68466813.9\n",
      "[3500] ELBO: 68516454.8\n",
      "[4000] ELBO: 68378551.5\n",
      "[4500] ELBO: 68383309.5\n",
      "[5000] ELBO: 68341340.2\n",
      "[5500] ELBO: 68337654.6\n",
      "[6000] ELBO: 68359000.4\n",
      "[6500] ELBO: 68324554.3\n",
      "[7000] ELBO: 68329634.5\n",
      "[7500] ELBO: 68352366.0\n",
      "[8000] ELBO: 68318056.4\n",
      "[8500] ELBO: 68318660.7\n",
      "[9000] ELBO: 68339623.0\n",
      "[9500] ELBO: 68323272.8\n"
     ]
    }
   ],
   "source": [
    "# Define guide function\n",
    "guide = AutoDiagonalNormal(nnet_interpretable_model)\n",
    "\n",
    "# Reset parameter values\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Define the number of optimization steps\n",
    "n_steps = 10000\n",
    "\n",
    "# Setup the optimizer\n",
    "adam_params = {\"lr\": 0.01}\n",
    "optimizer = ClippedAdam(adam_params)\n",
    "\n",
    "# Setup the inference algorithm\n",
    "elbo = Trace_ELBO(num_particles=1)\n",
    "svi = SVI(nnet_interpretable_model, guide, optimizer, loss=elbo)\n",
    "\n",
    "# Do gradient steps\n",
    "for step in range(n_steps):\n",
    "    elbo = svi.step(Xnn, ynn)\n",
    "    if step % 500 == 0:\n",
    "        print(\"[%d] ELBO: %.1f\" % (step, elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "# Make predictions for test set\n",
    "predictive = Predictive(nnet_interpretable_model, guide=guide, num_samples=1000,\n",
    "                        return_sites=(\"obs\", \"_RETURN\"))\n",
    "samples = predictive(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CorrCoef: 0.253\n",
      "MAE: 21.096\n",
      "RMSE: 40.379\n",
      "R2: 0.061\n"
     ]
    }
   ],
   "source": [
    "y_pred = samples[\"obs\"].mean(axis=0).detach().numpy()\n",
    "\n",
    "corr, mae, rae, rmse, r2 = compute_error(y_test, y_pred)\n",
    "print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" % (corr, mae, rmse, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25.944462  , 21.86322   , 20.650864  ,  0.74684227,  7.2856917 ,\n",
       "       26.555796  , -4.4102335 ,  7.89875   , 26.56263   , -2.0773768 ,\n",
       "        6.980639  ,  1.5553894 ,  5.4555364 ,  5.0229826 , 24.907972  ,\n",
       "       27.777182  , 17.622673  , 24.62341   , -1.7299864 , 22.799974  ,\n",
       "       20.951872  , -0.5423774 ,  5.0209975 , -2.041235  , 25.952087  ,\n",
       "        3.2022843 , 25.3463    ,  5.556955  ,  0.2402338 , 22.494467  ,\n",
       "       -0.23819456, -5.291641  ,  7.902377  , 27.774513  , 13.914909  ,\n",
       "       25.134142  ,  6.9894147 , 11.567175  , 24.481276  , 17.973888  ,\n",
       "        5.5174656 ,  9.114879  ,  6.379212  , 11.887705  ,  7.2909102 ,\n",
       "        5.217951  ,  5.243947  ,  6.0707335 ,  9.1124935 , 25.541658  ,\n",
       "       27.161118  , 20.362133  ,  4.2685227 ,  6.4045873 , 12.783459  ,\n",
       "        3.6597235 , 10.359882  ,  0.7501983 ,  7.0532994 , 23.406776  ,\n",
       "       29.376917  , 11.5654545 ,  4.6065245 , 26.142202  ,  4.877506  ,\n",
       "        5.026278  ,  1.2817012 , 25.535048  , 12.490388  ,  5.2406287 ,\n",
       "        7.2848163 , -3.9830482 ,  7.2902865 ,  1.2691952 ,  6.684822  ,\n",
       "       27.166044  ,  9.116912  , 18.22921   ,  9.438764  ,  6.0753465 ,\n",
       "        4.9479113 , 16.454054  , 10.400263  , 18.274239  , 12.787108  ,\n",
       "       -5.5312285 , 21.885809  , -4.1216607 ,  8.500216  ,  6.068613  ,\n",
       "       24.609043  , -3.5149806 , 14.1385565 , 32.422295  , 26.557636  ,\n",
       "        7.288975  , 24.922298  , 16.757479  , 26.142122  ,  5.5529013 ,\n",
       "       13.097126  ,  8.19802   , 27.164736  , 99.813126  ,  3.5238125 ,\n",
       "       -8.390884  ,  7.317582  ,  7.291277  , 26.56097   , 22.781298  ,\n",
       "        9.117724  ,  7.8911376 ,  4.417752  , 18.533613  ,  0.217806  ,\n",
       "        6.9860916 , 11.568637  ,  8.526674  ,  5.4914374 ,  3.847909  ,\n",
       "        1.5254894 ,  7.8922987 ,  0.13747646,  8.203456  ,  6.16521   ,\n",
       "       -2.6431267 , 24.729713  , 10.414289  , 20.363073  , 26.141653  ,\n",
       "       24.622253  , 11.260777  , 24.92547   , 96.157974  , 23.102528  ,\n",
       "       25.944588  , 11.925209  , 25.641415  ,  7.669257  , -4.4041796 ,\n",
       "       23.09895   , 18.580141  , 27.163149  ,  5.366911  , 12.787687  ,\n",
       "       10.41084   , 23.710634  , -2.280568  ,  5.1585927 ,  0.5600449 ,\n",
       "       17.971851  ,  0.13492699, -3.3675199 , -6.7474732 , 13.1395445 ,\n",
       "        6.9856844 ,  3.5041366 , -6.14394   , 10.655668  ,  6.1446147 ,\n",
       "        7.896183  ,  5.163982  , -3.6034126 , 21.577133  , 25.128477  ,\n",
       "        4.7208114 , -7.018024  ,  7.893868  , 17.060139  , 53.910366  ,\n",
       "       25.23546   ,  3.0537643 ,  5.632452  ,  6.7642074 , -4.1650405 ,\n",
       "       20.058239  ,  3.33374   ,  9.498105  , 26.251467  ,  2.4803407 ,\n",
       "       25.091536  , 22.488558  , 26.556889  , 21.173697  ,  7.9022627 ,\n",
       "       24.611868  , 13.057826  , 60.42661   , 11.932504  , 31.804756  ,\n",
       "       18.276203  , 17.671144  , 25.94852   ,  3.1608481 , 23.095747  ,\n",
       "        6.9841485 ,  3.962066  ,  6.3721347 , -1.4250177 , -1.3587602 ,\n",
       "       24.015327  , 26.863436  ,  7.9740148 , 12.1767845 , 11.629167  ,\n",
       "       25.09253   , 10.351387  , 17.971361  , 20.367168  , -1.8961909 ,\n",
       "        6.396817  ,  4.9477963 , 24.611391  , 27.469954  , -2.9944098 ,\n",
       "        7.2880373 , 25.946976  , 25.533546  , 23.210524  ,  4.7175107 ,\n",
       "       25.84403   , 25.94886   , 23.712824  ,  5.332014  ,  3.0473645 ,\n",
       "       22.964333  , -2.0812337 , 10.65109   , 11.57126   , 11.262708  ,\n",
       "        1.1048043 ,  8.568968  , 13.087564  , 22.389723  , 22.388552  ,\n",
       "        2.776172  , 17.059864  , -1.7299988 , 24.009571  , 13.700023  ,\n",
       "       19.121128  ,  7.0745277 , 24.477983  ,  1.3306539 , 13.998911  ,\n",
       "        3.3793013 ,  5.025489  , 22.998262  , 54.85317   ,  6.6769423 ,\n",
       "       -0.6747227 , 17.668438  ,  6.1317263 ,  4.11148   , 20.970673  ,\n",
       "        8.504997  ,  9.10903   ,  5.0664964 ,  8.530922  , 26.861422  ,\n",
       "        4.116995  ,  0.7485852 , 17.359838  ,  8.809565  , 13.398555  ,\n",
       "       13.095176  ,  3.8141544 , 23.407087  , 22.798323  ,  4.0117674 ,\n",
       "       10.656925  ,  7.594392  , 11.270954  ,  5.559347  , 13.394218  ,\n",
       "       -1.4202996 , -8.575615  ,  7.6781845 , -1.7308517 , 24.429905  ,\n",
       "        6.986252  ,  5.0239415 ,  4.03286   , 15.842101  , 26.257105  ,\n",
       "       21.169762  ,  0.54341364,  4.726748  ,  6.069469  , 16.405848  ,\n",
       "        3.075159  ,  1.3570529 , 20.668531  , 24.315847  ,  4.4186053 ,\n",
       "        6.675886  , 12.183115  ,  6.0744667 ,  5.3344913 ,  4.8990626 ,\n",
       "        5.1621537 , -9.6130705 , -9.003388  , 20.262117  ,  4.7225623 ,\n",
       "        4.1154995 , 15.190562  ,  3.7274761 ,  9.438595  , 17.32543   ,\n",
       "       23.404638  , 23.710447  ,  3.9389634 ,  6.9860783 , 26.863268  ,\n",
       "        7.6074862 , 14.929881  ,  6.376122  ,  6.680171  ,  0.80703443,\n",
       "       10.71648   , 25.334929  , 12.786937  , -3.5972342 ,  0.744693  ,\n",
       "       -7.3573847 , 19.456919  ,  6.072011  ,  4.110253  ,  5.157273  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb833273add3e7c60eb33c0608260b79a61e072ade6f02cc8d07b0a26eef8ab8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
